{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a82f55b-b0a2-46eb-b56f-931055ac94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd06e66-fce6-4367-a42f-2a8af2cb4e90",
   "metadata": {},
   "source": [
    "## Loading MNIST Dataset\n",
    "### Containing Training Samples = 60,000, Testing Samples = 10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23768130-1d50-4698-974e-e14bd5a69cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist # Contains 28x28 sized images of handwritten digits from 0 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6288d-560c-4e64-868d-b4f772c703a2",
   "metadata": {},
   "source": [
    "### Dividing into Training and Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce160a07-c506-4f16-91de-292d7b6e258b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb25dd-5d1c-4832-9d88-fc92c5bb78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2177dd-7ed6-4c4e-8d85-8a69ad6a2cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5e9e4-3f71-4e99-94c3-eaaee1d1b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(x_train[0])\n",
    "plt.show()\n",
    "plt.imshow(x_train[0], cmap= plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9fff3-a615-4c2e-8f08-f7a29a4f4723",
   "metadata": {},
   "source": [
    "### Checking the value of each pixel\n",
    "#### Before Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba1e50-4abb-4c32-ae5f-809ad2c397cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26512a-98ed-4ce3-a8c5-d27007f01892",
   "metadata": {},
   "source": [
    "#### As images are in Gray Level (1 Channel ==> 0 to 255), not Colored(RGB)\n",
    "#### Normalizing the Data | Pre-processing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b758b-aa33-484c-b060-2853c0d40666",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tf.keras.utils.normalize(x_train, axis = 1)\n",
    "x_test = tf.keras.utils.normalize(x_test, axis = 1)\n",
    "plt.imshow(x_train[0], cmap = plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195fe02d-82a6-4e8d-8f5d-4e257cdbe855",
   "metadata": {},
   "source": [
    "#### After Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a8e84-0b1c-4479-96b2-dae01dcaadf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f01c903-edda-4416-a376-8131cb2622a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d7627d-33da-4e9c-b770-5265c0d48d44",
   "metadata": {},
   "source": [
    "#### Resizing Image to make it suitable for applying Convolution Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1ec7a-0bef-4ddd-bfb2-ba4e9b023138",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "IMG_SIZE = 28\n",
    "x_trainr = np.array(x_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1) #Increaasing one dimension for kernel / filter operation\n",
    "x_testr = np.array(x_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1) #Increaasing one dimension for kernel / filter operation\n",
    "print(\"Training Samples Dimension\", x_trainr.shape)\n",
    "print(\"Testing Samples Dimension\", x_testr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6bcfa1-bc30-42de-9222-6f06e0fe9b29",
   "metadata": {},
   "source": [
    "### Creating a Deep Neural Network\n",
    "#### Training on 60,000 samples of MNIST Handwritten Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb0ca11-b2fe-42ec-a7fd-9ee1bc7c909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776cf42a-a85b-4070-8553-91620a998fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "### First Convolution Layer 0 1 2 3 (60000, 28, 28, 1)  28-3+1= 26x26\n",
    "model.add(Conv2D(64, (3,3), input_shape = x_trainr.shape[1:])) # Mention Input Layer Size only for first convolution layer\n",
    "model.add(Activation(\"relu\")) # Activation Function to make it non-linear, <0 will be removed and >0 will be allowed\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) # Maxpooling (Single Maximum value of 2x2),\n",
    "\n",
    "## 2nd Convolution Layer \n",
    "model.add(Conv2D(64, (3,3), input_shape = x_trainr.shape[1:])) # 2nd Convolution layer\n",
    "model.add(Activation(\"relu\")) # Activation Function\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) # Maxpooling\n",
    "\n",
    "## 3rd Convolution Layer \n",
    "model.add(Conv2D(64, (3,3), input_shape = x_trainr.shape[1:])) # 3rd Convolution layer\n",
    "model.add(Activation(\"relu\")) # Activation Function\n",
    "model.add(MaxPooling2D(pool_size=(2,2))) # Maxpooling\n",
    "\n",
    "## Fully Connected Layer - 1\n",
    "model.add (Flatten()) ## Before using fully connected layer, need to flatten (Converts 2D to 1D)\n",
    "model.add (Dense(64)) \n",
    "model.add (Activation(\"relu\"))\n",
    "\n",
    "## Fully Connected Layer - 2\n",
    "model.add (Dense(32)) \n",
    "model.add (Activation(\"relu\"))\n",
    "\n",
    "## Last Fully Connected Layer , Output must be equal to number of classes, 10(0-9)\n",
    "model.add (Dense(10))\n",
    "model.add (Activation(\"softmax\")) ##Activation function is changed to softmax (Class Probabilities)\n",
    "\n",
    "##For binary classification, there will be one neuron in dense layer and activation function must be sigmoid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60bf47-86b9-478c-abb8-546ca2cbd15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f96cd8b-dd97-428d-b11b-b93a0d3543de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Training Samples = \", len(x_trainr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91d555a-8211-49c7-9d67-6aa9292fba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec427893-70a5-4144-9469-914e0af5c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_trainr, y_train, epochs=10, validation_split= 0.3) ##Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece246eb-cde5-44a6-b7c1-d32f4ee0d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluating on testing data MNIST\n",
    "test_loss, test_acc = model.evaluate(x_testr, y_test)\n",
    "print(\"Test Loss on 10,000 test samples\", test_loss)\n",
    "print(\"Validation Accuracy on 10,000 test samples\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea9e1e3-ef46-40e3-b76d-345b59add8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict([x_testr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa84ccd-df31-498f-b996-a4a0484d41e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be168a5c-4828-420c-b440-7199cfb4fb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to understand, convert the predictions from One hot encoding, we need to use numpy for that\n",
    "print(np.argmax(predictions[0])) # this will return the maximum value index and find the value of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4498bba3-10ac-4014-858b-12f43e12b463",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491c111f-cb8e-413e-8b24-9d6cc2c22356",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(predictions[128]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa7a39-8786-4467-b162-631bcb12b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(x_test[128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d39c9-2639-4f4a-a5e7-2d74f4bea7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48667d-e30e-4ff7-83d0-0efb33912031",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('eight.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e777101-6b25-497f-a1aa-114797091968",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ed32a1-d1b9-41d5-849e-11b9b7e6a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61624264-d412-4aab-851a-68361b54a1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91f107-2b6f-4769-8805-32993a4a585a",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1abd00-969f-420e-9b29-52584536137b",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized = cv2.resize(gray, (28,28), interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceab841-52b4-4149-934a-5fb70b78e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96162c05-23b1-4081-b1ff-45fa84247633",
   "metadata": {},
   "outputs": [],
   "source": [
    "newing = tf.keras.utils.normalize(resized, axis=1) # 0 to 1 Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64738578-101c-48fb-8d89-5497b776442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "newing = np.array(newing).reshape(-1, IMG_SIZE, IMG_SIZE, 1) # kernel operation of convolution layer,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042284a-deae-4e55-8591-743f922e1a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "newing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09c0d15-9b76-4bca-8421-1dcfe229cfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23ee62-b01a-4bd5-9252-2e39c6e82ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(newing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1aa9fe-0323-4c50-961b-8d068e4927ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d94ef2c-2518-4b20-b057-3db2584f75fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import cv2\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "\n",
    "# Set the canvas size to 28x28 pixels\n",
    "canvas_size = 28\n",
    "screen = pygame.display.set_mode((canvas_size, canvas_size))\n",
    "\n",
    "# Set the title of the window\n",
    "pygame.display.set_caption(\"28x28 Drawing Canvas\")\n",
    "\n",
    "# Set the background color to black\n",
    "screen.fill((0, 0, 0))\n",
    "\n",
    "# Set drawing color to white\n",
    "draw_color = (255, 255, 255)\n",
    "brush_size = 1\n",
    "\n",
    "# Function to save the drawing as an image\n",
    "def save_drawing(surface, filename):\n",
    "    pygame.image.save(surface, filename)\n",
    "\n",
    "# Main loop\n",
    "running = True\n",
    "while running:\n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "\n",
    "        # Drawing with mouse\n",
    "        if pygame.mouse.get_pressed()[0]:\n",
    "            mouse_pos = pygame.mouse.get_pos()\n",
    "            pygame.draw.circle(screen, draw_color, mouse_pos, brush_size)\n",
    "\n",
    "        # Save the image when the user presses the 'S' key\n",
    "        if event.type == pygame.KEYDOWN:\n",
    "            if event.key == pygame.K_s:\n",
    "                save_drawing(screen, \"drawing.png\")\n",
    "                print(\"Drawing saved as drawing.png\")\n",
    "    \n",
    "    # Update the display\n",
    "    pygame.display.flip()\n",
    "\n",
    "# Quit pygame\n",
    "pygame.quit()\n",
    "\n",
    "# # Convert the saved image to grayscale for feeding to a model\n",
    "# image = cv2.imread(\"drawing.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# # Resize the image if needed (usually not needed since it's already 28x28)\n",
    "# # image = cv2.resize(image, (28, 28))\n",
    "\n",
    "# # Normalize the image (optional depending on your model's requirements)\n",
    "# image = image / 255.0\n",
    "\n",
    "# # Now `image` is ready to be fed into a model\n",
    "# print(\"Image ready for model input.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f16b970-3ee7-4f18-945c-e2d028839133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution\n",
    "img1 = cv2.imread('drawing.png')\n",
    "plt.imshow(img1)\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "resized1 = cv2.resize(gray1, (28,28), interpolation = cv2.INTER_AREA)\n",
    "newing1 = tf.keras.utils.normalize(resized1, axis=1) # 0 to 1 Scaling\n",
    "newing1 = np.array(newing1).reshape(-1, IMG_SIZE, IMG_SIZE, 1) # kernel operation of convolution layer,\n",
    "plt.imshow(resized1)\n",
    "predictions1 = model.predict(newing1)\n",
    "print(np.argmax(predictions1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeda554-f3e4-405b-b0a2-9e1cdd4c800f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
